[{"content":"Cohort tables offer valuable insights into customer behavior by grouping them based on shared attributes, in this case, acquisition date or date of first purchase. They are fantastic for identifying whether you have a retention problem or a user acquisition problem and can allow marketers to understand user behavior through experiments. Learning to read these tables can be tricky at first but effectively empowers you to analyze customer engagement, retention, and acquisition trends.\nSee cohort tables in Tableau public.\nThree Effective Reading Methods:\nScan Horizontally (Rows): Allows us to analyze a single cohort\u0026rsquo;s retention rates over time. Insight:: The first cohort\u0026rsquo;s retention rate declines from 37% to 27% across twelve months. This cohort started with 885 customers (100% retention), and 324 made additional purchases (37%) the following month. This pattern continues, with 286 (32%) making purchases two months later, and so on. We also see that the oldest customers (cohort 1) had the best retention rates despite a steady decline. Scan Vertically (Columns): Allows us to compare retention rates for various cohorts within a specific period. Visually scan, looking for columns with similar shades that decrease in intensity (lighter) in subsequent months. This signifies declining retention rates. Insight: For example, in this case, we see that the first two months after the initial purchase shows the most dramatic drop in retention so maybe developing a loyalty campaign around these months may address and halt this trend. In addition, we see that the e-commerce platform acquired far less first-time customers in Dec \u0026lsquo;11 than it did in Dec \u0026lsquo;10. Maybe we have exhausted our total addressable market (TAM) or maybe our acquisition strategies are no longer effective. This begs further investigation. Scan Diagonally: Analyze specific month retention rates across all cohorts (e.g., comparing November or Cohort Index 12 retention rates across all cohorts). Insight:: Sales in November rise relative to the previous period, revealing a November sales surge observed across all cohorts. This could be due to seasonal factors (perhaps, Thanksgiving or holiday shopping) or a successful marketing campaigns. As an analyst, these insights may fuel further investigation and optimization. Note: In cohort analysis, you can ignore the last cell since it\u0026rsquo;s the current month i.e. a partial time period. Since the current month is ongoing, data may not be complete or representative of that period.\nCohort analyses become even more powerful when drill down further into a specific attribute. For example, what if we identified that a targeted campaign promoting abandoned carts drove the increase in November sales? We could run another cohort analysis for returning customers that received mobile push notifications or emails. This can help us determine which strategy was more effective.\nCombining Insights and Identifying Opportunities:\nDeeper Analysis: Combining cohort data with domain knowledge provides valuable insights. For example, we can integrate our cohort numbers with purchase data to forecast sales. Understanding Limitations:\nData Availability: Insights might take time to emerge, as data collection can span months or even years. Generalizability: Cohort analysis focuses on specific groups with shared characteristics, and the derived insights may not be universally applicable to broader contexts. By mastering cohort table interpretation, you gain a powerful tool to decipher customer behavior and make informed business decisions, ultimately leading to improved customer engagement and retention.\nTo read the SQL code, check my Github repo here.\n","permalink":"//localhost:1313/posts/cohort-analysis/","summary":"Cohort tables offer valuable insights into customer behavior by grouping them based on shared attributes, in this case, acquisition date or date of first purchase. They are fantastic for identifying whether you have a retention problem or a user acquisition problem and can allow marketers to understand user behavior through experiments. Learning to read these tables can be tricky at first but effectively empowers you to analyze customer engagement, retention, and acquisition trends.","title":"Understanding the Cohort Table: A Step-by-Step Guide a"},{"content":" King County, WA Housing Insights Line Chart:\nThe line chart displays daily prices from May 1, 2014, to May 1, 2015, illustrating a general upward trend over the year with some fluctuations. Housing prices rise from an average of $0.5M to $1M by November 2014, peaking at $1.3M in May 2015. The zip codes with the highest average price, ranging from $0.9M to $1M, include 98109, 98105, 98004, and 98040. These zip codes, situated close to Lake Union, the only water body within city limits, are still connected to the mainland, possibly making them desirable locations for homes. This observation is supported by the fact that Vashon Island, despite being surrounded by water, is one of the cheapest places to own a home, likely due to its isolation from the mainland and possibly poorer infrastructure.\nHistogram:\nNow turning to the right-skewed1 histogram, we observe that few homes were either very cheap or very expensive, with most observations falling between 300K and 500K. The tail on the right pulls the mean slightly to the right, making the median (center point) less than the mean. This means the data has a higher frequency of lower values, with a relatively small number of high-priced homes pulling up the mean. Most houses have 3-4 bedrooms and between 1-2 bathrooms, indicating a right-skewed distribution.\nView vs. Condition Heatmap:\nIn May 2014, the darkest shaded areas on the heatmap, representing homes with the highest average real estate prices, are those in very good condition with a fair view. By May 2015, homes in average condition with excellent views fetched the highest average prices. Homebuyers were more willing to pay top dollar for a home with a good or excellent view, regardless of renovation costs. Unsurprisingly, houses in poor condition with no view had the lowest average prices.\nAdditional Considerations:\nLocation: The heat map doesn\u0026rsquo;t account for location, which can significantly impact a house\u0026rsquo;s value. A house in excellent condition with a decent view might be more expensive in a desirable location compared to a house with a breathtaking view in a less sought-after area.\nMarket Trends: Real estate market trends can influence pricing. Depending on the market, a specific view (e.g., waterfront) might be more or less valuable to buyers, affecting the price of a house regardless of its physical condition.\nWhy do I say the histogram is right-skewed? Aren\u0026rsquo;t most of the data points clustered to the left? In a left-skewed distribution (negative skew), there are more data points on the right side, making it tilt to the left with a longer tail. Imagine the midpoint of a seesaw with data points on either side representing their frequency (number of observations). When there are more data points on the left side, that side weighs the seesaw down, making it tilt to the right with a longer tail. This is called a right-skewed distribution.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"//localhost:1313/posts/king-county-wa-tableau/","summary":"King County, WA Housing Insights Line Chart:\nThe line chart displays daily prices from May 1, 2014, to May 1, 2015, illustrating a general upward trend over the year with some fluctuations. Housing prices rise from an average of $0.5M to $1M by November 2014, peaking at $1.3M in May 2015. The zip codes with the highest average price, ranging from $0.9M to $1M, include 98109, 98105, 98004, and 98040.","title":"Insights into King County, WA Housing Market: Trends, Views, and Considerations"},{"content":"Hi! My name is Stann-Omar Jones and I\u0026rsquo;m a data analyst. This blog is my playground for exploring and sharing discoveries in the ever-evolving world of data science.\nMy writing aims to be accessible to everyone, regardless of technical background. I\u0026rsquo;ll explain coding concepts alongside real-world applications, making them engaging and understandable.\nYou can find me at:\nstannomarjones\nk10sj02\nDisclaimer This is my personal website. The opinions expressed here are my own and do not reflect the views of my current or previous employers. If you would like to learn about the terms and policies, please visit this page.\n","permalink":"//localhost:1313/about/","summary":"Stann-Omar Jones, a Data Analyst fueled by a background in strategic finance and politics, uses data to uncover valuable insights. His inclusive blog blends technical topics with accessible explanations, making programming and data discoveries approachable for all.","title":"About"},{"content":"The following are a few projects that I have worked on in my free time. They are in no particular order.\nSQL Web Analytics for Political Advocacy Description: Utilized SQLite to delve deeper into user behavior, device segmentation, and referral sources through extensive exploratory data analysis. Implemented joins, window functions, and comparison operators to extract actionable insights that informed strategic decisions.\nStacks: SQLite, Tableau\nSkills: Exploratory data analysis, SQL querying with joins, window functions, and comparison operators\nSQL Online Commerce Cohort Analysis Description: Utilized SSMS and Azure Data Studio to uncover insights into customer behavior, including cohort retention analysis and percentage breakdowns over time for online commerce database. Techniques such as CTEs, Temporary Tables, Window Functions, and the PIVOT function are employed to handle data transformation, calculation, and presentation. Created cohort dashboard in Tableau. Ideal for understanding online commerce trends and optimizing business strategies.\nStacks: SQLite, Tableau\nSkills: CTEs, Temporary Tables, Window Functions, and the PIVOT function\nGlobal COVID-19 Trend Analysis Description: Built an ETL pipeline using MySQL and MySQL Workbench. Utilized SQLAlchemy in Python for data transformation, cleaning, and loading into a MySQL data warehouse. Performed exploratory data analytics on the data warehouse to identify global COVID-19 trends and visualized results in a Tableau dashboard.\nStacks: MySQL, MySQL Workbench, Python (pandas), Tableau\nSkills: ETL (Extract, Transform, Load), Data warehousing, Data cleaning and transformation, Exploratory data analysis (EDA), SQL, Data visualization\nPersonal blog Description: A blog (where you are right now!) where I write about what\u0026rsquo;s on my mind.\nStacks: HTML/CSS, Hugo, Papermod theme, and Github Actions\n8 Week SQL Challenge: Mastering PostgreSQL with Docker Project: Completed the 8 Week SQL Challenge (Data with Danny) using PostgreSQL and Docker. Analyzed and cleaned eight diverse datasets, including banking and sales, utilizing various SQL techniques like window functions and Common Table Expressions (CTEs).\nStacks: PostgreSQL, Docker\nSkills: Data analysis with SQL, Data cleaning and manipulation, Window functions, Common Table Expressions (CTEs), Containerization\nReal Estate Analytics: King County Housing Sales Dashboard Description: Developed an interactive dashboard for analyzing King County housing sales. Includes line chart for trends, histograms for distribution analysis, map for geographical patterns, heatmap for multi-dimensional insights.\nStacks: Tableau\nSkills: Data Visualization, Dashboard design, Interactive development, Data analysis\nPredicting Gender Self-Perception: An Analysis of American Masculinity Description: Preprocessed a national US survey dataset on masculinity, focusing on data loading, mapping, cleaning, and exploration. Applied predictive analytics, including regression analysis, random forests, and XGBoost, to predict the gender self-perception of survey respondents. Employed advanced machine learning techniques, such as feature engineering and hyperparameter tuning, to optimize model performance.\nStacks: Python (pandas, matplotlib, scikit-learn, geopandas)\nSkills: Data preprocessing, Data cleaning, Exploratory data analysis, Predictive modeling, Machine learning, Feature engineering, Hyperparameter tuning, Geospatial analysis\nBritish Columbia Labor Market Research Description: Processed CSV datasets containing Canadian provincial labor market statistics through import, cleaning, and merging. Created visualizations focusing on monthly employment changes across Canadian provinces, highlighting sectors like accommodation, food services, and retail trade.\nStacks: R (tidyverse, dplyr, lubridate)\nSkills: Data wrangling, Data cleaning and merging, Data visualization, Economic analysis\n","permalink":"//localhost:1313/projects/","summary":"Explore my personal projects on GitHub, including various SQL, Tableau and Python challenges.","title":"Projects"}]